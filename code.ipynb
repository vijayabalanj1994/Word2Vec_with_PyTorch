{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e158f3-835d-4057-b7fc-cdc74db8d60c",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d877ff-0389-48b5-b0ca-673062517dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijay\\AppData\\Local\\Temp\\ipykernel_18820\\2207896738.py:29: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, SVG\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.core.display import display, SVG\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a29cf-841d-4e47-ac0f-7421df818a56",
   "metadata": {},
   "source": [
    "#### Checking if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19171875-ef26-4146-bf5d-aefabdf59ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f402e5d-908b-4106-9dc0-94bd0e32e48f",
   "metadata": {},
   "source": [
    "#### Defing a funtion to plot word embeddings in 2d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26911e8e-34c9-46b5-8340-064fe7bbde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(word_embeddings, vocab=vocab):\n",
    "\n",
    "    # performing t-SNE on the embeddings to reduce their dimentionality to 2D using \"TSNE\" function from \"sklearn\" libary\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    word_embeddings_2d = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "    # plotting the results with labels from vocab\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i, word in enumerate(vocab.get_itos()):\n",
    "        plt.scatter(word_embeddings_2d[i,0], word_embeddings_2d[i,1])\n",
    "        plt.annotate(word, (word_embeddings_2d[i,0], word_embeddings_2d[i,1]))\n",
    "\n",
    "    plt.xlabel(\"t-SNE component 1\")\n",
    "    plt.ylabel(\"t-SNE component 2\")\n",
    "    plt.title(\"Word Embeddings visualized with t-SNE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fa2fe-c052-40f5-a666-adee3c9be188",
   "metadata": {},
   "source": [
    "#### Defining a function to return similar words to a given word by calculating Cosine distance~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd70ddc-7a3b-40d4-badd-1dea878efe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_word(word, word_embeddings, top_k=5):\n",
    "    if word not in word_embeddings:\n",
    "        print(\"Word not found in embeddings.\")\n",
    "        return []\n",
    "\n",
    "    target_embedding = word_embeddings[word]\n",
    "\n",
    "    # calculating cosine distange with all words\n",
    "    similarities = {}\n",
    "    for w, embedding in word_embeddings.items():\n",
    "        if w != word:\n",
    "            similarity = torch.dot(target_embedding, embedding) / (torch.norm(traget_embedding) * torch.norm(embedding))\n",
    "            similarities[w] = similarity.item()\n",
    "\n",
    "    # soting the similarities\n",
    "    sorted_similarities = sorted(similarities.items(), key= lambda x: x[1], reverse=True)\n",
    "\n",
    "    # returning the tok k similar words\n",
    "    most_similar_word = [w for w, _ in sorted_similarities[:tok_k]]\n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094127d-e914-4959-ad87-4380dce154c6",
   "metadata": {},
   "source": [
    "#### A toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4846b57-355d-4de3-aea0-fd6388a0dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "He had a huge smile on his face when he won the race\n",
    "The tiny kitten played with a fluffy toy mouse\n",
    "The team celebrated their victory with a grand parade\n",
    "She bought a small, delicate necklace for her sister\n",
    "The mountain peak stood majestic and tall against the clear blue sky\n",
    "The toddler took small, careful steps as she learned to walk\n",
    "The house had a spacious backyard with a big swimming pool\n",
    "He felt a sense of accomplishment after completing the challenging puzzle\n",
    "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
    "The children played happily in the small, cozy room\n",
    "The book had an enormous impact on readers around the world\n",
    "The wind blew gently, rustling the leaves of the tall trees\n",
    "She painted a beautiful, intricate design on the small canvas\n",
    "The concert hall was filled with thousands of excited fans\n",
    "The garden was adorned with colorful flowers of all sizes\n",
    "I hope to achieve great success in my chosen career path\n",
    "The skyscraper towered above the city, casting a long shadow\n",
    "He gazed in awe at the breathtaking view from the mountaintop\n",
    "The artist created a stunning masterpiece with bold brushstrokes\n",
    "The baby took her first steps, a small milestone that brought joy to her parents\n",
    "The team put in a tremendous amount of effort to win the championship\n",
    "The sun set behind the horizon, painting the sky in vibrant colors\n",
    "The professor gave a fascinating lecture on the history of ancient civilizations\n",
    "The house was filled with laughter and the sound of children playing\n",
    "She received a warm, enthusiastic welcome from the audience\n",
    "The marathon runner had incredible endurance and determination\n",
    "The child's eyes sparkled with excitement upon opening the gift\n",
    "The ship sailed across the vast ocean, guided by the stars\n",
    "The company achieved remarkable growth in a short period of time\n",
    "The team worked together harmoniously to complete the project\n",
    "The puppy wagged its tail, expressing its happiness and affection\n",
    "She wore a stunning gown that made her feel like a princess\n",
    "The building had a grand entrance with towering columns\n",
    "The concert was a roaring success, with the crowd cheering and clapping\n",
    "The baby took a tiny bite of the sweet, juicy fruit\n",
    "The athlete broke a new record, achieving a significant milestone in her career\n",
    "The sculpture was a masterpiece of intricate details and craftsmanship\n",
    "The forest was filled with towering trees, creating a sense of serenity\n",
    "The children built a small sandcastle on the beach, their imaginations running wild\n",
    "The mountain range stretched as far as the eye could see, majestic and awe-inspiring\n",
    "The artist's brush glided smoothly across the canvas, creating a beautiful painting\n",
    "She received a small token of appreciation for her hard work and dedication\n",
    "The orchestra played a magnificent symphony that moved the audience to tears\n",
    "The flower bloomed in vibrant colors, attracting butterflies and bees\n",
    "The team celebrated their victory with a big, extravagant party\n",
    "The child's laughter echoed through the small room, filling it with joy\n",
    "The sunflower stood tall, reaching for the sky with its bright yellow petals\n",
    "The city skyline was dominated by tall buildings and skyscrapers\n",
    "The cake was adorned with a beautiful, elaborate design for the special occasion\n",
    "The storm brought heavy rain and strong winds, causing widespread damage\n",
    "The small boat sailed peacefully on the calm, glassy lake\n",
    "The artist used bold strokes of color to create a striking and vivid painting\n",
    "The couple shared a passionate kiss under the starry night sky\n",
    "The mountain climber reached the summit after a long and arduous journey\n",
    "The child's eyes widened in amazement as the magician performed his tricks\n",
    "The garden was filled with the sweet fragrance of blooming flowers\n",
    "The basketball player made a big jump and scored a spectacular slam dunk\n",
    "The cat pounced on a small mouse, displaying its hunting instincts\n",
    "The mansion had a grand entrance with a sweeping staircase and chandeliers\n",
    "The raindrops fell gently, creating a rhythmic patter on the roof\n",
    "The baby took a big step forward, encouraged by her parents' applause\n",
    "The actor delivered a powerful and emotional performance on stage\n",
    "The butterfly fluttered its delicate wings, mesmerizing those who watched\n",
    "The company launched a small-scale advertising campaign to test the market\n",
    "The building was constructed with strong, sturdy materials to withstand earthquakes\n",
    "The singer's voice was powerful and resonated throughout the concert hall\n",
    "The child built a massive sandcastle with towers, moats, and bridges\n",
    "The garden was teeming with a variety of small insects and buzzing bees\n",
    "The athlete's muscles were well-developed and strong from years of training\n",
    "The sun cast long shadows as it set behind the mountains\n",
    "The couple exchanged heartfelt vows in a beautiful, intimate ceremony\n",
    "The dog wagged its tail vigorously, a sign of excitement and happiness\n",
    "The baby let out a tiny giggle, bringing joy to everyone around\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec892828-d3d2-4b26-b48f-c8e0965521b5",
   "metadata": {},
   "source": [
    "#### Tokenizing and building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c7b41b-cda8-49eb-b4bb-a030d81d407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:-    I wish I was little bit taller\n",
      "Tokenized Document:-  ['i', 'wish', 'i', 'was', 'little', 'bit', 'taller']\n",
      "Token Indices:-       [20, 108, 20, 7, 272, 136, 376]\n"
     ]
    }
   ],
   "source": [
    "# The \"basic_english\" tokenizer from \"torchtext\" library\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# A function to get tokenized text for one document at a time\n",
    "def tokenize_data(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "# tokenizing the entire toy dataset\n",
    "tokenized_toy_data = tokenizer(toy_data)\n",
    "\n",
    "# building the vocabulary using 'bulid_vocab_from_iterator' function from \"torchtext\" library\n",
    "vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # This index will be returned when OOV token is queried\n",
    "\n",
    "# printing the tokenized text and token indices of 1st example document\n",
    "print(\"Example Document:-   \",toy_data.split('\\n')[0])\n",
    "print(\"Tokenized Document:- \",tokenizer(toy_data.split('\\n')[0]))\n",
    "print(\"Token Indices:-      \", vocab(tokenizer(toy_data.split('\\n')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d68103-9fae-4411-9e01-b2b2eb12461c",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeecb5eb-aa1e-4392-9ae4-bc504812cdef",
   "metadata": {},
   "source": [
    "#### Pre-processing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25be62cf-7646-4a91-81f6-0d82e3d382d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example trining data:- ([......Context words......], Traget word)\n",
      "                       (['i', 'around', 'i', 'was'], 'wish')\n",
      "Token Indices       :- ([20, 51, 20, 7], 108)\n"
     ]
    }
   ],
   "source": [
    "# generating target and context traing data\n",
    "CONTEXT_SIZE = 2\n",
    "cobow_data = []\n",
    "for i in range(1, len(tokenized_toy_data)-CONTEXT_SIZE):\n",
    "    \n",
    "    context = (\n",
    "        [tokenized_toy_data[i-j-1] for j in range(CONTEXT_SIZE)]+\n",
    "        [tokenized_toy_data[i+j+1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "\n",
    "    target = tokenized_toy_data[i]\n",
    "    cobow_data.append((context, target))\n",
    "\n",
    "print(\"Example trining data:- ([......Context words......], Traget word)\")\n",
    "print(f\"                       {cobow_data[0]}\")   \n",
    "print(f\"Token Indices       :- ({vocab(cobow_data[0][0])}, {vocab[cobow_data[0][1]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecccb664-7072-46ac-9a3e-0245064e4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 traning data tensors -\n",
      "\n",
      "offsets_tensor :-\n",
      "\n",
      " tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36], device='cuda:0')\n",
      "\n",
      "\n",
      "context_tensor:-\n",
      "\n",
      " tensor([ 20,  51,  20,   7, 108,  20,   7, 272,  20, 108, 272, 136,   7,  20,\n",
      "        136, 376, 272,   7, 376,  20, 136, 272,  20, 108, 376, 136, 108,  20,\n",
      "         20, 376,  20,   7, 108,  20,   7,   2,  20, 108,   2, 133],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "target_tensor:-\n",
      "\n",
      " tensor([108,  20,   7, 272, 136, 376,  20, 108,  20,   7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# a function to convert the pre-processed data into tensors for each \"batch\" from the \"dataloader\"\n",
    "def collate_batch(batch):\n",
    "    target_list, context_list, offsets = [], [], [0]\n",
    "\n",
    "    for _context, _target in batch:\n",
    "        target_list.append(vocab[_target])\n",
    "        context_tensor = torch.tensor(vocab(_context), dtype=torch.int64)\n",
    "        context_list.append(context_tensor)\n",
    "        offsets.append(context_tensor.size(0))\n",
    "\n",
    "    target_list_tensor = torch.tensor(target_list, dtype=torch.int64)\n",
    "    context_list_tensor = torch.cat(context_list)\n",
    "    offesets_tensor = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    return target_list_tensor.to(device), context_list_tensor.to(device), offesets_tensor.to(device)\n",
    "\n",
    "# processing the first 10 traning data using the collate_batch function\n",
    "target_tensor, context_tensor, offsets_tensor = collate_batch(cobow_data[0:10])\n",
    "print(\"The first 10 traning data tensors -\")\n",
    "print(\"\\noffsets_tensor :-\\n\\n\", offsets_tensor)\n",
    "print(\"\\n\\ncontext_tensor:-\\n\\n\", context_tensor)\n",
    "print(\"\\n\\ntarget_tensor:-\\n\\n\", target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4533ae2-21e2-4686-ab6c-b202d5e46520",
   "metadata": {},
   "source": [
    "#### Creating dataloaders for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f6cbee6-1a60-4827-bdd4-b5298a4c4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# creating dataloaders using \"DataLoader\" function from \"pytorch\" library\n",
    "dataloader_cbow = DataLoader(\n",
    "    cobow_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbac48-1a02-4353-971f-739671712fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
